#!/usr/bin/env python3
"""
GPUå‹åŠ›æµ‹è¯•å·¥å…· - å¯æŒ‡å®šæ˜¾å­˜åˆ©ç”¨ç‡

ä½¿ç”¨æ–¹æ³•:
  python gpu_stress.py                    # é»˜è®¤80%æ˜¾å­˜åˆ©ç”¨ç‡ï¼Œæ‰€æœ‰GPU
  python gpu_stress.py -m 60              # 60%æ˜¾å­˜åˆ©ç”¨ç‡ï¼Œæ‰€æœ‰GPU  
  python gpu_stress.py -m 90 -g 0,1       # 90%æ˜¾å­˜åˆ©ç”¨ç‡ï¼Œä»…GPU 0å’Œ1
  python gpu_stress.py --memory 50 --gpu 2 # 50%æ˜¾å­˜åˆ©ç”¨ç‡ï¼Œä»…GPU 2

ç›‘æ§å‘½ä»¤:
  nvidia-smi -l 1                         # å®æ—¶ç›‘æ§
  kill $(cat /tmp/gpu_stress_main.pid)    # åœæ­¢ç¨‹åº
"""

import torch
import time
from datetime import datetime
import os
import signal
import psutil
import numpy as np

def get_gpu_memory():
    """è·å–æ¯ä¸ªGPUçš„æ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
    torch.cuda.synchronize()
    return [torch.cuda.memory_allocated(i)/1024**2 for i in range(torch.cuda.device_count())]

def get_gpu_utilization():
    """è·å–GPUä½¿ç”¨ç‡"""
    try:
        import pynvml
        pynvml.nvmlInit()
        utilization = []
        for i in range(torch.cuda.device_count()):
            handle = pynvml.nvmlDeviceGetHandleByIndex(i)
            util = pynvml.nvmlDeviceGetUtilizationRates(handle)
            utilization.append(util.gpu)
        return utilization
    except:
        return [0] * torch.cuda.device_count()

def stress_gpu(gpu_id, matrix_size=8192, sleep_time=0, memory_usage_percentage=80):
    """åœ¨æŒ‡å®šGPUä¸Šè¿è¡ŒçŸ©é˜µä¹˜æ³•è¿ç®—ï¼Œå¯æŒ‡å®šæ˜¾å­˜åˆ©ç”¨ç‡"""
    torch.cuda.set_device(gpu_id)
    
    # è·å–GPUæ˜¾å­˜ä¿¡æ¯
    gpu_memory_total = torch.cuda.get_device_properties(gpu_id).total_memory / 1024**2  # MB
    gpu_memory_reserved = torch.cuda.memory_reserved(gpu_id) / 1024**2  # MB
    available_memory = gpu_memory_total - gpu_memory_reserved - 512  # é¢„ç•™512MBä½œä¸ºç¼“å†²
    
    # æ ¹æ®æŒ‡å®šç™¾åˆ†æ¯”è®¡ç®—ç›®æ ‡æ˜¾å­˜ä½¿ç”¨é‡
    target_memory_usage = available_memory * (memory_usage_percentage / 100.0)
    
    print(f'GPU {gpu_id} æ€»æ˜¾å­˜: {gpu_memory_total:.2f} MB, å¯ç”¨æ˜¾å­˜: {available_memory:.2f} MB')
    print(f'ç›®æ ‡æ˜¾å­˜åˆ©ç”¨ç‡: {memory_usage_percentage}% ({target_memory_usage:.2f} MB)')
    
    # åŠ¨æ€è®¡ç®—æœ€ä¼˜çŸ©é˜µå¤§å°
    bytes_per_element = 4  # float32
    
    # è®¡ç®—çŸ©é˜µé…ç½®
    num_main_matrices = max(6, int(memory_usage_percentage / 10))  # æ ¹æ®ç™¾åˆ†æ¯”è°ƒæ•´çŸ©é˜µæ•°é‡
    num_temp_matrices = max(3, int(memory_usage_percentage / 20))
    total_matrices = num_main_matrices + num_temp_matrices
    
    memory_per_matrix = (target_memory_usage * 1024**2) / total_matrices  # å­—èŠ‚
    elements_per_matrix = memory_per_matrix / bytes_per_element
    optimal_size = int(np.sqrt(elements_per_matrix))
    
    # ç¡®ä¿çŸ©é˜µå¤§å°åˆç†
    optimal_size = max(min(optimal_size, 16384), 1024)  # é™åˆ¶åœ¨1024-16384ä¹‹é—´
    
    print(f'GPU {gpu_id} ä½¿ç”¨çŸ©é˜µå¤§å°: {optimal_size}x{optimal_size}')
    print(f'çŸ©é˜µé…ç½®: {num_main_matrices}ä¸ªä¸»çŸ©é˜µ + {num_temp_matrices}ä¸ªä¸´æ—¶çŸ©é˜µ')
    
    # åˆ›å»ºçŸ©é˜µ
    matrices = []
    for i in range(num_main_matrices):
        matrix = torch.randn(optimal_size, optimal_size, dtype=torch.float32, device=f'cuda:{gpu_id}')
        matrices.append(matrix)
    
    # åˆ›å»ºé¢å¤–çš„å¤§çŸ©é˜µç”¨äºæŒç»­è®¡ç®—
    large_matrix_a = torch.randn(optimal_size, optimal_size, dtype=torch.float32, device=f'cuda:{gpu_id}')
    large_matrix_b = torch.randn(optimal_size, optimal_size, dtype=torch.float32, device=f'cuda:{gpu_id}')
    
    # åˆ›å»ºä¸´æ—¶è®¡ç®—ç©ºé—´
    temp_results = [torch.empty(optimal_size, optimal_size, dtype=torch.float32, device=f'cuda:{gpu_id}') 
                   for _ in range(num_temp_matrices)]
    
    print(f'GPU {gpu_id} åˆå§‹æ˜¾å­˜å ç”¨: {get_gpu_memory()[gpu_id]:.2f} MB')
    
    # åˆ›å»ºçŠ¶æ€æ–‡ä»¶
    pid = os.getpid()
    status_file = f'/tmp/gpu_stress_{gpu_id}_{pid}.status'
    with open(status_file, 'w') as f:
        f.write('running')
    
    iteration_count = 0
    start_time = time.time()
    
    try:
        while True:
            # æ£€æŸ¥çŠ¶æ€æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(status_file):
                print(f'GPU {gpu_id} çŠ¶æ€æ–‡ä»¶è¢«åˆ é™¤ï¼Œç¨‹åºé€€å‡º')
                break
                
            # æ‰§è¡Œå¯†é›†çš„çŸ©é˜µè¿ç®—
            for i in range(len(matrices)-1):
                temp_idx = i % len(temp_results)
                
                # çŸ©é˜µä¹˜æ³•è¿ç®—é“¾
                torch.matmul(matrices[i], matrices[i+1], out=temp_results[temp_idx])
                temp_results[temp_idx] = torch.relu(temp_results[temp_idx])
                temp_results[temp_idx] = torch.sigmoid(temp_results[temp_idx])
                temp_results[temp_idx] = torch.tanh(temp_results[temp_idx])
                
                # ä¸å¤§å‹çŸ©é˜µè¿ç®—
                temp_result2 = torch.matmul(large_matrix_a, temp_results[temp_idx])
                matrices[i] = torch.add(temp_results[temp_idx], temp_result2)
            
            # é¢å¤–çš„æŒç»­è®¡ç®—è´Ÿè½½
            for _ in range(3):
                temp = torch.matmul(large_matrix_a, large_matrix_b)
                large_matrix_a = torch.relu(temp)
                large_matrix_b = torch.sigmoid(temp)
            
            iteration_count += 1
            
            # æ¯1000æ¬¡è¿­ä»£è®°å½•ä¸€æ¬¡çŠ¶æ€
            if iteration_count % 1000 == 0:
                current_time = time.time()
                elapsed = current_time - start_time
                if elapsed >= 60:  # æ¯åˆ†é’Ÿæ‰“å°ä¸€æ¬¡çŠ¶æ€
                    mem_used = get_gpu_memory()[gpu_id]
                    mem_percentage = (mem_used / gpu_memory_total) * 100
                    util = get_gpu_utilization()[gpu_id]
                    print(f'[{datetime.now()}] GPU {gpu_id} - æ˜¾å­˜: {mem_used:.2f}MB ({mem_percentage:.1f}%), ä½¿ç”¨ç‡: {util}%, è¿­ä»£: {iteration_count}')
                    start_time = current_time
            
            # åªåœ¨å¿…è¦æ—¶åŒæ­¥
            if iteration_count % 5000 == 0:
                torch.cuda.synchronize(gpu_id)
                
    finally:
        # æ¸…ç†çŠ¶æ€æ–‡ä»¶
        if os.path.exists(status_file):
            os.remove(status_file)

def main():
    import argparse
    
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description='GPUå‹åŠ›æµ‹è¯•å·¥å…· - å¯æŒ‡å®šæ˜¾å­˜åˆ©ç”¨ç‡')
    parser.add_argument('--memory', '-m', type=int, default=80, 
                       help='æ˜¾å­˜åˆ©ç”¨ç‡ç™¾åˆ†æ¯” (10-95), é»˜è®¤80%%')
    parser.add_argument('--gpu', '-g', type=str, default='all',
                       help='æŒ‡å®šGPU ID (0,1,2...) æˆ– "all" ä½¿ç”¨æ‰€æœ‰GPU, é»˜è®¤all')
    
    args = parser.parse_args()
    
    # éªŒè¯å‚æ•°
    if args.memory < 10 or args.memory > 95:
        print('âŒ æ˜¾å­˜åˆ©ç”¨ç‡å¿…é¡»åœ¨10-95%ä¹‹é—´')
        return
    
    # è·å–å¯ç”¨çš„GPUæ•°é‡
    num_gpus = torch.cuda.device_count()
    print(f'å‘ç° {num_gpus} ä¸ªGPUè®¾å¤‡')
    
    # ç¡®å®šè¦ä½¿ç”¨çš„GPU
    if args.gpu == 'all':
        target_gpus = list(range(num_gpus))
    else:
        try:
            target_gpus = [int(x.strip()) for x in args.gpu.split(',')]
            # éªŒè¯GPU ID
            for gpu_id in target_gpus:
                if gpu_id >= num_gpus or gpu_id < 0:
                    print(f'âŒ GPU ID {gpu_id} æ— æ•ˆï¼Œå¯ç”¨GPU: 0-{num_gpus-1}')
                    return
        except ValueError:
            print(f'âŒ GPUå‚æ•°æ ¼å¼é”™è¯¯ï¼Œä½¿ç”¨æ ¼å¼: 0,1,2 æˆ– all')
            return
    
    print(f'ğŸ“Š é…ç½®ä¿¡æ¯:')
    print(f'   - ç›®æ ‡æ˜¾å­˜åˆ©ç”¨ç‡: {args.memory}%')
    print(f'   - ä½¿ç”¨GPU: {target_gpus}')
    
    # ä¿å­˜ä¸»è¿›ç¨‹PID
    with open('/tmp/gpu_stress_main.pid', 'w') as f:
        f.write(str(os.getpid()))
    
    # åœ¨æŒ‡å®šGPUä¸Šå¯åŠ¨è¿›ç¨‹
    child_pids = []
    for gpu_id in target_gpus:
        pid = os.fork()
        if pid == 0:  # å­è¿›ç¨‹
            try:
                print(f'åœ¨ GPU {gpu_id} ä¸Šå¯åŠ¨å‹åŠ›æµ‹è¯• (æ˜¾å­˜ç›®æ ‡: {args.memory}%)')
                stress_gpu(gpu_id, memory_usage_percentage=args.memory)
            except KeyboardInterrupt:
                print(f'GPU {gpu_id} æµ‹è¯•ç»ˆæ­¢')
            finally:
                os._exit(0)
        else:
            child_pids.append(pid)
    
    print(f'æ‰€æœ‰GPUè¿›ç¨‹å·²å¯åŠ¨ï¼Œä¸»è¿›ç¨‹PID: {os.getpid()}')
    print('è¦åœæ­¢ç¨‹åºï¼Œè¯·è¿è¡Œ: kill $(cat /tmp/gpu_stress_main.pid)')
    
    try:
        # ä¸»è¿›ç¨‹ç­‰å¾…æ‰€æœ‰å­è¿›ç¨‹
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print('æ”¶åˆ°ç»ˆæ­¢ä¿¡å·ï¼Œæ­£åœ¨æ¸…ç†...')
    finally:
        # æ¸…ç†æ‰€æœ‰å­è¿›ç¨‹
        for pid in child_pids:
            try:
                os.kill(pid, signal.SIGTERM)
            except:
                pass
        # æ¸…ç†PIDæ–‡ä»¶
        if os.path.exists('/tmp/gpu_stress_main.pid'):
            os.remove('/tmp/gpu_stress_main.pid')
        print('ç¨‹åºå·²ç»ˆæ­¢')

if __name__ == '__main__':
    main()