#!/usr/bin/env python3
"""
Flash Attention æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯Flash Attentionæ˜¯å¦èƒ½åœ¨å½“å‰ç¯å¢ƒä¸­æ­£å¸¸è¿è¡Œ
"""

import sys
import subprocess
import importlib
import torch
import warnings
warnings.filterwarnings('ignore')

def check_python_version():
    """æ£€æŸ¥Pythonç‰ˆæœ¬"""
    print("=" * 50)
    print("æ£€æŸ¥Pythonç‰ˆæœ¬...")
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    if sys.version_info < (3, 7):
        print("âŒ éœ€è¦Python 3.7æˆ–æ›´é«˜ç‰ˆæœ¬")
        return False
    print("âœ… Pythonç‰ˆæœ¬ç¬¦åˆè¦æ±‚")
    return True

def check_pytorch():
    """æ£€æŸ¥PyTorchå®‰è£…å’ŒCUDAå¯ç”¨æ€§"""
    print("=" * 50)
    print("æ£€æŸ¥PyTorch...")
    try:
        print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
        
        if torch.cuda.is_available():
            print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
            print(f"GPUè®¾å¤‡: {torch.cuda.get_device_name(0)}")
            print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        else:
            print("âš ï¸ æœªæ£€æµ‹åˆ°CUDAï¼ŒFlash Attentionéœ€è¦CUDAæ”¯æŒ")
            
        print("âœ… PyTorchæ£€æŸ¥å®Œæˆ")
        return True
    except Exception as e:
        print(f"âŒ PyTorchæ£€æŸ¥å¤±è´¥: {e}")
        return False

def install_package(package_name):
    """å®‰è£…æŒ‡å®šçš„PythonåŒ…"""
    try:
        print(f"æ­£åœ¨å®‰è£… {package_name}...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", package_name])
        print(f"âœ… {package_name} å®‰è£…æˆåŠŸ")
        return True
    except subprocess.CalledProcessError:
        print(f"âŒ {package_name} å®‰è£…å¤±è´¥")
        return False

def check_flash_attn():
    """æ£€æŸ¥Flash Attentionæ˜¯å¦å¯ç”¨"""
    print("=" * 50)
    print("æ£€æŸ¥Flash Attention...")
    
    # å°è¯•å¯¼å…¥flash_attn
    try:
        flash_attn_available = importlib.util.find_spec("flash_attn") is not None
        if not flash_attn_available:
            print("æœªæ‰¾åˆ°flash_attnåŒ…ï¼Œå°è¯•å®‰è£…...")
            if not install_package("flash-attn"):
                print("âŒ Flash Attentionå®‰è£…å¤±è´¥")
                return False
                
        import flash_attn
        print(f"Flash Attentionç‰ˆæœ¬: {flash_attn.__version__}")
        print("âœ… Flash Attentionå®‰è£…æˆåŠŸ")
        
        # è¿è¡Œç®€å•çš„æµ‹è¯•
        print("è¿è¡Œç®€å•æµ‹è¯•...")
        test_flash_attention()
        
        print("âœ… Flash Attentionæµ‹è¯•é€šè¿‡")
        return True
        
    except Exception as e:
        print(f"âŒ Flash Attentionæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_flash_attention():
    """è¿è¡Œç®€å•çš„Flash Attentionæµ‹è¯•"""
    try:
        from flash_attn import flash_attn_func
        import torch.nn.functional as F
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size, seq_len, n_heads, head_dim = 2, 64, 4, 64
        dtype = torch.float16
        
        # åœ¨CUDAä¸Šåˆ›å»ºéšæœºå¼ é‡
        q = torch.randn(batch_size, seq_len, n_heads, head_dim, dtype=dtype, device='cuda') * 0.01
        k = torch.randn(batch_size, seq_len, n_heads, head_dim, dtype=dtype, device='cuda') * 0.01
        v = torch.randn(batch_size, seq_len, n_heads, head_dim, dtype=dtype, device='cuda') * 0.01
        
        # ä½¿ç”¨Flash Attentionè®¡ç®—
        output = flash_attn_func(q, k, v, causal=True)
        
        # ä½¿ç”¨æ™®é€šAttentionè®¡ç®—ï¼ˆç”¨äºéªŒè¯ï¼‰
        with torch.backends.cuda.sdp_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):
            expected = F.scaled_dot_product_attention(
                q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2), 
                attn_mask=None, dropout_p=0.0, is_causal=True
            ).transpose(1, 2)
        
        # æ£€æŸ¥ç»“æœæ˜¯å¦ç›¸ä¼¼ï¼ˆå…è®¸ä¸€å®šçš„è¯¯å·®ï¼‰
        diff = (output - expected).abs().mean()
        print(f"Flash Attentionä¸æ ‡å‡†Attentionçš„å¹³å‡å·®å¼‚: {diff.item():.6f}")
        
        if diff < 0.1:
            print("âœ… Flash Attentionè®¡ç®—ç»“æœæ­£ç¡®")
        else:
            print("âš ï¸ Flash Attentionè®¡ç®—ç»“æœä¸é¢„æœŸæœ‰è¾ƒå¤§å·®å¼‚")
            
    except Exception as e:
        print(f"âŒ Flash Attentionæµ‹è¯•è¿è¡Œå¤±è´¥: {e}")
        raise

def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯•Flash Attention...")
    
    # æ£€æŸ¥ç¯å¢ƒ
    success = True
    success &= check_python_version()
    success &= check_pytorch()
    
    if success:
        success &= check_flash_attn()
    
    print("=" * 50)
    if success:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡! Flash Attentionå¯ä»¥æ­£å¸¸è¿è¡Œ")
        return 0
    else:
        print("âŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®")
        return 1

if __name__ == "__main__":
    sys.exit(main())